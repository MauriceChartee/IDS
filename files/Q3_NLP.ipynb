{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0af5273",
   "metadata": {},
   "source": [
    "## Question 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99996cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses = [\n",
    "\"nlp/01-introduction.csv\",\n",
    "\"nlp/02-data-exploration.csv\",\n",
    "\"nlp/03-decision-trees.csv\",\n",
    "\"nlp/04-regression.csv\",\n",
    "\"nlp/05-support-vector-machines.csv\",\n",
    "\"nlp/06-neural-networks-1.csv\",\n",
    "\"nlp/07-neural-networks-2.csv\",\n",
    "\"nlp/08-evaluation.csv\",\n",
    "\"nlp/09-clustering.csv\",\n",
    "\"nlp/10-frequent-itemsets.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42af17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dfs = []\n",
    "for csv_path in corpuses:\n",
    "    csv_file = Path(csv_path)\n",
    "    if not csv_file.exists():\n",
    "        # fallback if the notebook is executed from the repository root\n",
    "        csv_file = Path(\"files\") / csv_file\n",
    "\n",
    "    lecture = csv_file.stem  # e.g., 01-introduction\n",
    "    part = pd.read_csv(csv_file)\n",
    "    part[\"lecture\"] = lecture\n",
    "    dfs.append(part)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df[[\"lecture\", \"start\", \"end\", \"text\"]]\n",
    "\n",
    "print(\"Full dataframe shape (before tokenization):\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56211750",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b59e62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "# a) 25 most frequent words (whitespace split, no preprocessing)\n",
    "all_words = [w for txt in df[\"text\"].astype(str) for w in txt.split()]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "top25_words = word_counts.most_common(25)\n",
    "top25_df = pd.DataFrame(top25_words, columns=[\"word\", \"count\"])\n",
    "\n",
    "fig = px.bar(\n",
    "    top25_df,\n",
    "    x=\"word\",\n",
    "    y=\"count\",\n",
    "    title=\"Top 25 most frequent words (whitespace split, no preprocessing)\",\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n",
    "\n",
    "print(\"Two problems with this approach (visible in the histogram):\")\n",
    "print(\n",
    "    \"1) Stopwords/filler words dominate (e.g., 'the', 'and'), making the plot less informative. \"\n",
    "    \"Fix: remove stopwords (and optionally filler words).\"\n",
    ")\n",
    "print(\n",
    "    \"2) Casing and punctuation create duplicates (e.g., 'Data' vs 'data', 'data,' vs 'data'). \"\n",
    "    \"Fix: lowercase text and remove punctuation before tokenization.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea498d0",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "# Download required NLTK resources if needed (punkt_tab is required by the task)\n",
    "for resource, path in [\n",
    "    (\"punkt_tab\", \"tokenizers/punkt_tab\"),\n",
    "    (\"punkt\", \"tokenizers/punkt\"),\n",
    "    (\"stopwords\", \"corpora/stopwords\"),\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punct_table = str.maketrans({p: \" \" for p in string.punctuation})\n",
    "\n",
    "def preprocess_and_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Lowercase, remove punctuation, and tokenize using NLTK (punkt_tab/punkt).\"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(punct_table)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return [t for t in nltk.word_tokenize(text) if t]\n",
    "\n",
    "# Add tokenized text column\n",
    "df[\"tokenized_text\"] = df[\"text\"].apply(preprocess_and_tokenize)\n",
    "print(\"Full dataframe shape (after tokenization):\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# 25 most frequent tokens in 01-introduction (excluding stopwords)\n",
    "intro_tokens = [\n",
    "    t\n",
    "    for toks in df.loc[df[\"lecture\"] == \"01-introduction\", \"tokenized_text\"]\n",
    "    for t in toks\n",
    "    if t not in stop_words\n",
    "]\n",
    "intro_counts = Counter(intro_tokens)\n",
    "top25_intro = intro_counts.most_common(25)\n",
    "top25_intro_df = pd.DataFrame(top25_intro, columns=[\"token\", \"count\"])\n",
    "\n",
    "fig = px.bar(\n",
    "    top25_intro_df,\n",
    "    x=\"token\",\n",
    "    y=\"count\",\n",
    "    title=\"Top 25 most frequent tokens in 01-introduction (no stopwords)\",\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46ae1c",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20423b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# c) Stacked histogram for selected tokens, grouped by lecture\n",
    "selected_tokens = [\n",
    "    \"data\",\n",
    "    \"decision\",\n",
    "    \"predict\",\n",
    "    \"derivative\",\n",
    "    \"network\",\n",
    "    \"easy\",\n",
    "    \"database\",\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for lecture, group in df.groupby(\"lecture\"):\n",
    "    flat = [t for toks in group[\"tokenized_text\"] for t in toks]\n",
    "    c = Counter(flat)\n",
    "    for tok in selected_tokens:\n",
    "        rows.append({\"lecture\": lecture, \"token\": tok, \"count\": c[tok]})\n",
    "\n",
    "freq_df = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.bar(\n",
    "    freq_df,\n",
    "    x=\"token\",\n",
    "    y=\"count\",\n",
    "    color=\"lecture\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Token frequencies across lectures (stacked by lecture)\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Helper: for each token, show the lecture where it occurs most\n",
    "top_lectures_per_token = (\n",
    "    freq_df.loc[freq_df.groupby(\"token\")[\"count\"].idxmax()][[\"token\", \"lecture\", \"count\"]]\n",
    "    .sort_values(\"token\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "display(top_lectures_per_token)\n",
    "\n",
    "print(\n",
    "    \"Observation: 'data' appears in almost all lectures, while more topic-specific tokens are concentrated in the corresponding lectures \"\n",
    "    \"(e.g., 'decision' in decision trees, 'network' in neural networks). \"\n",
    "    \"Some terms like 'predict' occur across multiple modeling lectures.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d023045",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d14919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import random\n",
    "\n",
    "# d) N-gram language model\n",
    "\n",
    "def build_ngram_model(n: int, token_sequences) -> ConditionalFreqDist:\n",
    "    \"\"\"Builds a ConditionalFreqDist mapping (n-1)-token contexts to next-token counts.\"\"\"\n",
    "    cfd = ConditionalFreqDist()\n",
    "    for tokens in token_sequences:\n",
    "        padded = [\"<s>\"] * (n - 1) + list(tokens) + [\"</s>\"]\n",
    "        for gram in ngrams(padded, n):\n",
    "            context = tuple(gram[:-1])\n",
    "            nxt = gram[-1]\n",
    "            cfd[context][nxt] += 1\n",
    "    return cfd\n",
    "\n",
    "\n",
    "def predict_next_word(model: ConditionalFreqDist, context: tuple[str, ...]) -> str | None:\n",
    "    \"\"\"Samples the next word given a context; returns None if context unseen.\"\"\"\n",
    "    if context not in model:\n",
    "        return None\n",
    "\n",
    "    dist = model[context]\n",
    "    if len(dist) == 0:\n",
    "        return None\n",
    "\n",
    "    candidates = sorted(dist.keys())\n",
    "    weights = [dist[w] for w in candidates]\n",
    "\n",
    "    # Required: seed before each random choice for reproducibility\n",
    "    random.seed(32133)\n",
    "    return random.choices(candidates, weights=weights, k=1)[0]\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    seed_text: str,\n",
    "    n: int,\n",
    "    model: ConditionalFreqDist,\n",
    "    max_new_tokens: int = 30,\n",
    ") -> list[str]:\n",
    "    \"\"\"Generates up to max_new_tokens after the seed text.\"\"\"\n",
    "    seed_tokens = preprocess_and_tokenize(seed_text)\n",
    "    history = [\"<s>\"] * max(0, (n - 1) - len(seed_tokens)) + seed_tokens.copy()\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = tuple(history[-(n - 1) :]) if n > 1 else tuple()\n",
    "        next_word = predict_next_word(model, context)\n",
    "        if next_word is None or next_word == \"</s>\":\n",
    "            break\n",
    "        generated.append(next_word)\n",
    "        history.append(next_word)\n",
    "\n",
    "    return seed_tokens + generated\n",
    "\n",
    "\n",
    "seed = \"introduction to data\"\n",
    "for n in [3, 4, 5, 24]:\n",
    "    model = build_ngram_model(n, df[\"tokenized_text\"])\n",
    "    generated_tokens = generate_text(seed, n, model, max_new_tokens=30)\n",
    "    print(f\"n={n}:\", \" \".join(generated_tokens))\n",
    "\n",
    "print(\n",
    "    \"\\nComment: For smaller n (3â€“5), the model has many seen contexts, so it tends to generate longer (but sometimes less coherent) text. \"\n",
    "    \"For large n (e.g., 24), contexts become very specific and often unseen, so generation frequently stops early due to sparsity. \"\n",
    "    \"For n > 5 we generally expect more sparsity (fewer observed contexts), leading to shorter generations or verbatim continuation of very frequent phrases.\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ea8fe",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from IPython.display import display\n",
    "\n",
    "# e) Hierarchical TF-IDF timestamp retrieval\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tfidf_analyzer(doc: str) -> list[str]:\n",
    "    \"\"\"Takes a pre-tokenized doc (space-separated), removes stopwords and stems.\"\"\"\n",
    "    tokens = doc.split()\n",
    "    return [stemmer.stem(t) for t in tokens if t and t not in stop_words]\n",
    "\n",
    "\n",
    "def cosine_scores(doc_matrix, query_vector):\n",
    "    # With l2-normalized TF-IDF vectors, dot product = cosine similarity\n",
    "    return doc_matrix.dot(query_vector.T).toarray().ravel()\n",
    "\n",
    "\n",
    "def lecture_documents(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    # One document per lecture: concatenate all tokens of its segments\n",
    "    return dataframe.groupby(\"lecture\")[\"tokenized_text\"].apply(\n",
    "        lambda rows: \" \".join([t for toks in rows for t in toks])\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_timestamps(query: str, k: int = 2, m: int = 2):\n",
    "    query_doc = \" \".join(preprocess_and_tokenize(query))\n",
    "\n",
    "    # Level 1: retrieve lectures\n",
    "    lec_docs = lecture_documents(df)\n",
    "    vec_lecture = TfidfVectorizer(analyzer=tfidf_analyzer)\n",
    "    X_lecture = vec_lecture.fit_transform(lec_docs.values)\n",
    "    q_lecture = vec_lecture.transform([query_doc])\n",
    "    lec_scores = cosine_scores(X_lecture, q_lecture)\n",
    "\n",
    "    top_lec_idx = lec_scores.argsort()[::-1][:k]\n",
    "    results = []\n",
    "\n",
    "    for idx in top_lec_idx:\n",
    "        lecture_name = lec_docs.index[idx]\n",
    "        lecture_score = float(lec_scores[idx])\n",
    "\n",
    "        # Level 2: retrieve segments within this lecture\n",
    "        segs = df.loc[df[\"lecture\"] == lecture_name].reset_index(drop=True)\n",
    "        seg_docs = segs[\"tokenized_text\"].apply(lambda toks: \" \".join(toks)).tolist()\n",
    "\n",
    "        vec_seg = TfidfVectorizer(analyzer=tfidf_analyzer)\n",
    "        X_seg = vec_seg.fit_transform(seg_docs)\n",
    "        q_seg = vec_seg.transform([query_doc])\n",
    "        seg_scores = cosine_scores(X_seg, q_seg)\n",
    "\n",
    "        top_seg_idx = seg_scores.argsort()[::-1][:m]\n",
    "        timestamps = []\n",
    "        for j in top_seg_idx:\n",
    "            timestamps.append(\n",
    "                {\n",
    "                    \"start\": float(segs.loc[j, \"start\"]),\n",
    "                    \"end\": float(segs.loc[j, \"end\"]),\n",
    "                    \"score\": float(seg_scores[j]),\n",
    "                    \"text\": segs.loc[j, \"text\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"lecture\": lecture_name,\n",
    "                \"score\": lecture_score,\n",
    "                \"timestamps\": timestamps,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "query_comments = {\n",
    "    \"gradient descent approach\": (\n",
    "        \"Expect hits in optimization/backprop sections; top segments should mention gradients, step sizes, or loss minimization.\"\n",
    "    ),\n",
    "    \"beer and diapers\": (\n",
    "        \"Classic market-basket example; expect the frequent itemsets/association rules lecture, with segments referencing support/confidence.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "queries = [\"gradient descent approach\", \"beer and diapers\"]\n",
    "for q in queries:\n",
    "    res = retrieve_timestamps(q, k=2, m=2)\n",
    "    print(f\"Query: {q}\")\n",
    "    for r in res:\n",
    "        print(f\"- {r['lecture']} (lecture_score={r['score']:.4f})\")\n",
    "        for ts in r[\"timestamps\"]:\n",
    "            print(\n",
    "                f\"  - [{ts['start']:.2f}, {ts['end']:.2f}] seg_score={ts['score']:.4f}: {ts['text'][:140]}...\"\n",
    "            )\n",
    "\n",
    "    print(\"Comment:\", query_comments.get(q, \"Segments should reflect the query terms.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
